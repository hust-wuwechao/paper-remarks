但是数据流计算模型和参数服务器计算模型的一致性模型不尽相同，参数服务器的一致性模型比如BSP或者SSP都会打破数据流原有的异步计算逻辑。参数服务器和数据流结合的灾备策略和一致性管理策略需要仔细的设计才能很好地统一和融合。
为了兼顾运行效率和易用性，机器学习模型训练计算框架的编程语言的选择一般采用前后端分离的方式：以C/C++、Java/Scala等作为后端以保证系统运行效率，
使用Python、R等作为前端以提供更为易用的编程接口。对于后端语言的选择上，主流的就是Java和C++，这两者各有优劣：
在生态上，Java由于易于开发使得其生态要远远好于C++，很多大数据计算框架都基于Java或者类Java语言开发；在可移植性上，
由于JVM屏蔽了很多底层差异性，所以Java要优于C++；在内存管理上，基于GC的Java在大数据、同步分布式并行的情况下，效率要远低于优化过的C++的效率，
因为大数据情况下，GC的概率会很高，而一旦一台服务器开始GC其计算能力将受很大影响，整体集群尤其在同步情况下的计算效率也会大打折扣，而机器数增加的情况下
，在一定时刻触发GC的概率也会大大增加；在语言抽象上，C++的模板机制在编译时刻进行展开，可以做更多的编译优化，在实际执行时除了产生的程序文件更大一些之外，
整体执行效率非常高，而与之对应的Java泛型采用类型擦除方式实现，在实际运行时做数据类型cast，会带来很多额外的开销，使得其整体执行效率受到很大影响。
执行效率的优化
执行效率优化方面主要举例分享计算、存储、通讯、容错四个方面的优化。
在计算方面，最重要的优化点就是均衡。均衡不仅包括不同的机器、不同的计算线程之间的负载均衡，还包括算术逻辑运算资源、存储资源、通讯资源等等各种与计算有关资
源之间的均衡，其最终目的是最大化所有计算资源的利用率。在实际的优化过程中，需要仔细地对程序进行Profiling，然后找出可能的性能瓶颈，针对性能瓶颈进行优化，
解决瓶颈问题，但是这时候性能瓶颈可能会转移，就要继续迭代：Profilingà发现瓶颈à解决瓶颈。
 
典型的计算性能优化循环
 
CPU和GPU的架构对比
分布式计算是有代价的，比如序列化代价、网络通讯代价等等，并不是所有的任务都需要分布式执行，有些情况下任务或者任务的某些部分可以很好地被单机执行，
不要为了分布式而分布式。为了得到更好的计算性能，需要对单机和分布式进行分离优化。（任务应该如何切分以及应该使用几台机器这都是大问题所在。。。。。）
CPU、GPU、FPGA等不同硬件有各自的优势，比如CPU适合复杂指令，有分支预测，大缓存，适合任务并行；GPU有大量的算术逻辑运算单元，但缓存较小，没有分支预测，
适合粗粒度数据并行，但不适合复杂指令执行，可以用来加速比如矩阵运算等粗粒度并行的计算任务；FPGA对于特定的计算任务，比如深度学习预测，
经过优化后有着介于CPU和GPU之间的峰值，同时功耗远低于GPU芯片。针对机器学习任务需要进行合理的任务调度，充分发挥不同计算硬件的优势，提升计算硬件的利用率。

近些年CPU、GPU等计算硬件的效率提升速度远高于主存性能的提升速度，所以计算和存储上的性能差距在不断扩大，形成了“存储墙”（Memory Wall），因此在很多问题上，存储优化更为重要。在存储方面，从CPU的寄存器到L1、L2等高速缓存，再到CPU本地内存，再到其他CPU内存，还有外存等有着复杂的存储结构和不同的存储硬件，访问效率也有着量级的差距。Jeff Dean建议编程人员牢记不同存储硬件的性能数据。
 
存储层级架构、性能数据和存储墙
针对存储的层次结构和各个层级存储硬件的性能特性，可以采取数据本地化及访存模式等存储优化的策略。因为机器学习是迭代的，可以将一些训练数据或者一些中间计算
结果放在本地，再次训练时，无需请求远端的数据；另外在单机情况下，也可以尝试不同的内存分配策略，调整计算模式，增强数据本地化。在访存模式优化方面，也可以
进行很多优化：数据访问重新排序，比如GPU中纹理渲染和矩阵乘法运算中常见的Z秩序曲线优化；调整数据布局，比如可以采用更紧致的数据结构，提升顺序访存的缓存命
中率，同时，在多线程场景下，尽量避免线程之间频繁竞争申请释放内存，会竞争同一把锁。除此之外还可以将冷热数据进行分离，提升缓存命中率；数据预取，比如可以
用另外一根线程提前预取数据到更快的存储中，提升后续计算的访存效率。
通信是分布式机器学习计算系统中至关重要的部分。通讯包括点对点通讯和组通讯（如AllReduce、AllGather等）。可通过软件优化、硬件优化的形式提高执行效率。
在软件优化方面，可以通过比如序列化框架优化、通讯压缩、应用层优化的方式进行优化：
通讯依赖于序列化，通用序列化框架比如ProtoBuffer、Thrift等，为了通用性、一些前后兼容性和跨语言考虑等会牺牲一定的效率，针对特定的通讯场景可以
设计更加简单的序列化框架，提升序列化效率。在带宽成为瓶颈时，可以考虑使用CPU兑换带宽的方式，比如利用压缩技术来降低带宽压力。更重要的优化来自于
考虑应用层通讯模式，可以做更多的优化：比如参数服务器的客户端，可以将同一台机器中多个线程的请求进行请求合并，因为同一次机器学习训练过程中，
不同线程之间大概率会有很多重复的模型参数请求；或者根据参数服务器不同的一致性模型，可以做请求缓存，提升查询效率，降低带宽；或者对于不同的网络
拓扑，可以采取不同的组通讯实现方式。
除了软件优化之外，通讯架构需要充分利用硬件特性，利用硬件来提升网络吞吐、降低网络延迟，比如可以配置多网卡建立冗余链路提升网络吞吐，或者部署 
Infiniband提升网络吞吐、降低网络延迟等。
在容错方面，对于不同的系统，容错策略之间核心的区别就在于选择最适合的Tradeoff。这里的Tradeoff是指每次失败后恢复任务所需要付出的代价和为了
降低这个代价所付出的overhead之间的权衡。在选择机器学习模型训练系统的容错策略时，需要考虑机器学习模型训练任务的特点：首先机器学习模型训练
是一个迭代式的计算任务，中间状态较多；其次机器学习模型训练系统中模型参数是最重要的状态；最后，机器学习模型训练不一定需要强一致性。
在业界常见的有Data Lineage和Checkpointing两种机器学习训练任务灾备方案。Data Lineage通过记录数据的来源，简化了对数据来源的追踪，一旦
数据发生错误或者丢失，可以根据Data Lineage找到之前的数据利用重复计算进行数据恢复，常见的开源项目Spark就使用这种灾备方案。Data Lineage的粒度
可大可小，同时需要一个比较可靠的维护Data Lineage的服务，总体overhead较大，对于机器学习模型训练中的共享状态——模型参数不一定是很好的灾备方式，
因为模型参数是共享的有着非常多的中间状态，每个中间状态都依赖于之前版本的模型参数和中间所有数据的计算；与Data Lineage不同，机器学习模型训练系统
中的Checkpointing策略，一般会重点关注对机器学习模型参数的灾备，由于机器学习是迭代式的，可以利用这一点，在满足机器学习一致性模型的情况下，
在单次或多次迭代之间或者迭代内对机器学习模型参数以及训练进度进行灾备，这样在发生故障的情况下，可以从上一次迭代的模型checkpoint开始，
进行下一轮迭代。相比于Data Lineage，机器学习模型训练系统对模型参数和模型训练进度进行Checkpointing灾备是更加自然和合适的，所以目前主流的专门
针对机器学习设计的计算框架比如Tensorflow、Mxnet等都是采用Checkpointing灾备策略。
 
除了上述的容错方式之外，还可以使用传统灾备常用的部署冗余系统来进行灾备，根据灾备系统的在线情况，可以分为冷、温和热备份方式，实际应用中可以根据实际
的资源和计算性能要求选择最合适实际问题的冗余容错方式。
